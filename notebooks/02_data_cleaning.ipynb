{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2f8026-a829-44bc-94ac-20d774178c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD HAPPINESS REPORT - DATA CLEANING\n",
      "\n",
      "Loading raw dataset...\n",
      "Raw dataset loaded: 1,502 rows × 11 columns\n",
      "Working copy created for cleaning\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: SETUP AND IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"WORLD HAPPINESS REPORT - DATA CLEANING\")\n",
    "\n",
    "# Loading the raw dataset\n",
    "print(\"\\nLoading raw dataset...\")\n",
    "df_raw = pd.read_csv('../data/raw/world_happiness.csv', sep=';', decimal=',')\n",
    "print(f\"Raw dataset loaded: {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
    "\n",
    "# Creating a copy for cleaning (preserving original dataset)\n",
    "df = df_raw.copy()\n",
    "print(\"Working copy created for cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcbaeae-7ef3-46d2-acb6-1af27abec508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values summary:\n",
      "Regional indicator: 3 missing values (0.2%)\n",
      "Ranking: No missing values\n",
      "Country: No missing values\n",
      "Happiness score: No missing values\n",
      "GDP per capita: No missing values\n",
      "Social support: No missing values\n",
      "Healthy life expectancy: No missing values\n",
      "Freedom to make life choices: No missing values\n",
      "Generosity: No missing values\n",
      "Perceptions of corruption: No missing values\n",
      "Year: No missing values\n",
      "\n",
      "Duplicate country-year combinations: 0\n",
      "\n",
      "Data Types:\n",
      "  Ranking: int64\n",
      "  Country: object\n",
      "  Regional indicator: object\n",
      "  Happiness score: float64\n",
      "  GDP per capita: float64\n",
      "  Social support: float64\n",
      "  Healthy life expectancy: int64\n",
      "  Freedom to make life choices: float64\n",
      "  Generosity: float64\n",
      "  Perceptions of corruption: float64\n",
      "  Year: int64\n",
      "\n",
      "POTENTIAL DATA QUALITY ISSUES:\n",
      "Happiness scores in valid range: 1.721 to 7.842\n",
      "GDP per capita: No negative values\n",
      "Social support: No negative values\n",
      "Healthy life expectancy: No negative values\n",
      "Freedom to make life choices: No negative values\n",
      "Generosity: No negative values\n",
      "\n",
      "Value Range Analysis:\n",
      "  Happiness score: 1.721 to 7.842\n",
      "  GDP per capita: 0.000 to 10.000\n",
      "  Social support: 0.000 to 1.000\n",
      "  Healthy life expectancy: 39.000 to 85.000\n",
      "  Freedom to make life choices: 0.000 to 1.000\n",
      "  Generosity: 0.000 to 1.000\n",
      "  Perceptions of corruption: 0.000 to 1.000\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: INITIAL DATA QUALITY ASSESSMENT\n",
    "# Identifying data quality issues that need to be addressed\n",
    "print('Missing values summary:')\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_percent =( missing_summary / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column' : missing_summary.index,\n",
    "    'Missing Count': missing_summary.values,\n",
    "    'Missing Percentage': missing_percent.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "for _, row in missing_df.iterrows():\n",
    "    if row['Missing Count'] > 0:\n",
    "        print(f\"{row['Column']}: {row['Missing Count']} missing values ({row['Missing Percentage']:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{row['Column']}: No missing values\")\n",
    "\n",
    "# Checking for duplicates\n",
    "duplicates = df.duplicated(['Country', 'Year']).sum()\n",
    "print(f\"\\nDuplicate country-year combinations: {duplicates}\")\n",
    "\n",
    "# Checking data types\n",
    "print(f\"\\nData Types:\")\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "# Checking for obvious data quality issues\n",
    "print(f\"\\nPOTENTIAL DATA QUALITY ISSUES:\")\n",
    "\n",
    "# 1. Checking happiness score range\n",
    "happiness_range = [df['Happiness score'].min(), df['Happiness score'].max()]\n",
    "if happiness_range[0] < 0 or happiness_range[1] > 10:\n",
    "    print(f\"Happiness scores outside 0-10 range: {happiness_range[0]:.3f} to {happiness_range[1]:.3f}\")\n",
    "else:\n",
    "    print(f\"Happiness scores in valid range: {happiness_range[0]:.3f} to {happiness_range[1]:.3f}\")\n",
    "\n",
    "# 2. Checking for negative values in columns that shouldn't have them\n",
    "negative_cols = ['GDP per capita', 'Social support', 'Healthy life expectancy', \n",
    "                'Freedom to make life choices', 'Generosity']\n",
    "\n",
    "for col in negative_cols:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"{col}: {negative_count} negative values\")\n",
    "        else:\n",
    "            print(f\"{col}: No negative values\")\n",
    "\n",
    "# 3. Checking for unrealistic values\n",
    "print(f\"\\nValue Range Analysis:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col not in ['Ranking', 'Year']:\n",
    "        min_val, max_val = df[col].min(), df[col].max()\n",
    "        print(f\"  {col}: {min_val:.3f} to {max_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09128941-4bd9-4161-8f9e-28cefc957e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HANDLING MISSING VALUES\n",
      "Addressing missing values...\n",
      "\n",
      "After missing value handling:\n",
      "  Dataset shape: 1,502 rows × 11 columns\n",
      "  Total missing values: 3\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: HANDLING MISSING VALUES\n",
    "# Cleaning or imputing missing data based on patterns found\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nHANDLING MISSING VALUES\")\n",
    "\n",
    "# Since our initial assessment showed no missing values, we'll verify this\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"No missing values found - dataset is complete!\")\n",
    "    print(\"No missing value imputation needed\")\n",
    "else:\n",
    "    print(\"Addressing missing values...\")\n",
    "    \n",
    "    # Strategy for missing values (if any exist):\n",
    "    # 1. For happiness scores: Cannot impute (too important) - would remove rows\n",
    "    # 2. For other metrics: Could use median by region or forward fill\n",
    "    \n",
    "    # Example missing value handling (uncomment if needed):\n",
    "    # # Remove rows with missing happiness scores\n",
    "    # df = df.dropna(subset=['Happiness score'])\n",
    "    # \n",
    "    # # Impute other missing values with regional medians\n",
    "    # for col in ['GDP per capita', 'Social support', 'Healthy life expectancy']:\n",
    "    #     if col in df.columns and df[col].isnull().sum() > 0:\n",
    "    #         df[col] = df.groupby('Regional indicator')[col].transform(\n",
    "    #             lambda x: x.fillna(x.median())\n",
    "    #         )\n",
    "\n",
    "print(f\"\\nAfter missing value handling:\")\n",
    "print(f\"  Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"  Total missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbde746-0260-4f88-b893-c5431ebeaf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STANDARDIZING COUNTRY NAMES\n",
      "Analyzing country name consistency...\n",
      "Expected years per country: 10\n",
      "Countries with fewer than 10 years of data:\n",
      "  Djibouti: 1 years\n",
      "  Congo: 1 years\n",
      "  Puerto Rico: 1 years\n",
      "  Oman: 1 years\n",
      "  State of Palestine: 1 years\n",
      "  Trinidad & Tobago: 1 years\n",
      "  Algeria: 2 years\n",
      "  Maldives: 2 years\n",
      "  Suriname: 2 years\n",
      "  Somaliland region: 2 years\n",
      "... and 44 more countries\n",
      "\n",
      "Checking for similar country names...\n",
      "Potential naming inconsistencies found:\n",
      "  Bosnia and Herzegovina similar to: ['Trinidad and Tobago']\n",
      "  Central African Republic similar to: ['Czech Republic', 'Dominican Republic']\n",
      "  China similar to: ['Hong Kong S.A.R. of China', 'Taiwan Province of China']\n",
      "  Congo similar to: ['Congo (Brazzaville)', 'Congo (Kinshasa)']\n",
      "  Congo (Brazzaville) similar to: ['Congo (Kinshasa)']\n",
      "\n",
      "No country name standardization needed\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: STANDARDIZING COUNTRY NAMES\n",
    "# Ensuring consistent country naming across years\n",
    "print(\"\\nSTANDARDIZING COUNTRY NAMES\")\n",
    "\n",
    "# Checking for potential country name inconsistencies\n",
    "print(\"Analyzing country name consistency...\")\n",
    "\n",
    "# Looking for countries with varying counts across years (potential name changes)\n",
    "country_year_counts = df.groupby('Country')['Year'].count().sort_values()\n",
    "expected_years = df['Year'].nunique()\n",
    "\n",
    "print(f\"Expected years per country: {expected_years}\")\n",
    "print(f\"Countries with fewer than {expected_years} years of data:\")\n",
    "\n",
    "inconsistent_countries = country_year_counts[country_year_counts < expected_years]\n",
    "if len(inconsistent_countries) > 0:\n",
    "    for country, count in inconsistent_countries.head(10).items():\n",
    "        print(f\"  {country}: {count} years\")\n",
    "    print(f\"... and {len(inconsistent_countries) - 10} more countries\" if len(inconsistent_countries) > 10 else \"\")\n",
    "else:\n",
    "    print(\"All countries have consistent representation\")\n",
    "\n",
    "# Checking for similar country names that might be duplicates\n",
    "print(f\"\\nChecking for similar country names...\")\n",
    "countries = sorted(df['Country'].unique())\n",
    "\n",
    "# Looking for potential naming issues\n",
    "naming_issues = []\n",
    "for i, country in enumerate(countries):\n",
    "    # Checking for countries with very similar names\n",
    "    similar = [c for c in countries[i+1:] if \n",
    "              len(set(country.lower().split()) & set(c.lower().split())) > 0]\n",
    "    if similar:\n",
    "        naming_issues.append((country, similar))\n",
    "\n",
    "if naming_issues:\n",
    "    print(\"Potential naming inconsistencies found:\")\n",
    "    for main_country, similar_countries in naming_issues[:5]:  # Show first 5\n",
    "        print(f\"  {main_country} similar to: {similar_countries}\")\n",
    "else:\n",
    "    print(\"No obvious country name inconsistencies found\")\n",
    "\n",
    "# Manual country name standardization (if needed)\n",
    "country_name_mapping = {\n",
    "    # Add any mappings here if inconsistencies are found\n",
    "    # 'Old Name': 'New Name',\n",
    "    # 'United States of America': 'United States',\n",
    "    # 'Republic of Korea': 'South Korea',\n",
    "}\n",
    "\n",
    "if country_name_mapping:\n",
    "    print(f\"\\nApplying country name standardization...\")\n",
    "    df['Country'] = df['Country'].replace(country_name_mapping)\n",
    "    print(f\"{len(country_name_mapping)} country names standardized\")\n",
    "else:\n",
    "    print(\"\\nNo country name standardization needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c405dd-cacf-4b68-96c4-79aa34fee14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTLIER DETECTION AND HANDLING\n",
      "Detecting outliers using IQR method...\n",
      "\n",
      "Happiness score:\n",
      "  Valid range: 2.066 to 8.806\n",
      "  Outliers found: 2\n",
      "  Outlier values: [np.float64(1.721), np.float64(1.859)]\n",
      "  Sample outlier cases:\n",
      "    Afghanistan (2023): 1.859\n",
      "    Afghanistan (2024): 1.721\n",
      "\n",
      "GDP per capita:\n",
      "  Valid range: -1.132 to 13.556\n",
      "  Outliers found: 0\n",
      "\n",
      "Social support:\n",
      "  Valid range: 0.119 to 1.307\n",
      "  Outliers found: 24\n",
      "  Outlier values: [np.float64(0.0), np.float64(0.0145), np.float64(0.04198), np.float64(0.04592), np.float64(0.04829), np.float64(0.04981), np.float64(0.05329), np.float64(0.05507), np.float64(0.0794), np.float64(0.07965), np.float64(0.08805), np.float64(0.09327), np.float64(0.0998), np.float64(0.10054), np.float64(0.11284)]\n",
      "  Sample outlier cases:\n",
      "    Central African Republic (2015): 0.000\n",
      "    Togo (2015): 0.100\n",
      "    Afghanistan (2016): 0.093\n",
      "    Benin (2016): 0.088\n",
      "    Togo (2016): 0.000\n",
      "\n",
      "Healthy life expectancy:\n",
      "  Valid range: 47.000 to 87.000\n",
      "  Outliers found: 19\n",
      "  Outlier values: [np.int64(39), np.int64(40), np.int64(41), np.int64(42), np.int64(44), np.int64(45), np.int64(46)]\n",
      "  Sample outlier cases:\n",
      "    Nigeria (2015): 45.000\n",
      "    Mozambique (2015): 42.000\n",
      "    Lesotho (2015): 41.000\n",
      "    Swaziland (2015): 41.000\n",
      "    Congo (Kinshasa) (2015): 42.000\n",
      "\n",
      "Freedom to make life choices:\n",
      "  Valid range: 0.092 to 1.276\n",
      "  Outliers found: 21\n",
      "  Outlier values: [np.float64(0.0), np.float64(0.00967), np.float64(0.01618), np.float64(0.02031), np.float64(0.02241), np.float64(0.02278), np.float64(0.03429), np.float64(0.04096), np.float64(0.04614), np.float64(0.071), np.float64(0.08951), np.float64(0.091)]\n",
      "  Sample outlier cases:\n",
      "    Iraq (2015): 0.000\n",
      "    Angola (2016): 0.010\n",
      "    Burundi (2016): 0.071\n",
      "    Sudan (2016): 0.000\n",
      "    Sudan (2017): 0.023\n",
      "\n",
      "Generosity:\n",
      "  Valid range: -0.155 to 0.781\n",
      "  Outliers found: 23\n",
      "  Outlier values: [np.float64(0.78721), np.float64(0.80918), np.float64(0.80999), np.float64(0.81414), np.float64(0.82926), np.float64(0.85533), np.float64(0.85851), np.float64(0.88037), np.float64(0.9491), np.float64(0.95232), np.float64(0.96061), np.float64(0.9663), np.float64(0.99602), np.float64(1.0)]\n",
      "  Sample outlier cases:\n",
      "    Myanmar (2015): 1.000\n",
      "    Myanmar (2016): 1.000\n",
      "    Myanmar (2017): 1.000\n",
      "    Indonesia (2018): 0.810\n",
      "    Myanmar (2018): 1.000\n",
      "\n",
      "Perceptions of corruption:\n",
      "  Valid range: -0.777 to 1.719\n",
      "  Outliers found: 0\n",
      "\n",
      "OUTLIER HANDLING STRATEGY:\n",
      "For this analysis, we'll keep outliers because:\n",
      "  1. Happiness data reflects real-world extremes\n",
      "  2. Some countries genuinely have extreme values\n",
      "  3. Outliers provide valuable insights\n",
      "  4. We have sufficient data points\n",
      "\n",
      "Outlier Summary:\n",
      "  Total outliers across all columns: 89\n",
      "  Percentage of dataset: 0.85%\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: HANDLING OUTLIERS\n",
    "# Identifying and address extreme values that might be data errors\n",
    "\n",
    "print(\"\\nOUTLIER DETECTION AND HANDLING\")\n",
    "\n",
    "# Defining columns to check for outliers\n",
    "outlier_cols = ['Happiness score', 'GDP per capita', 'Social support', \n",
    "               'Healthy life expectancy', 'Freedom to make life choices', \n",
    "               'Generosity', 'Perceptions of corruption']\n",
    "\n",
    "outliers_found = {}\n",
    "\n",
    "print(\"Detecting outliers using IQR method...\")\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in df.columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers_found[col] = len(outliers)\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Valid range: {lower_bound:.3f} to {upper_bound:.3f}\")\n",
    "        print(f\"  Outliers found: {len(outliers)}\")\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            print(f\"  Outlier values: {sorted(outliers[col].unique())}\")\n",
    "            # Show which countries have outliers\n",
    "            outlier_countries = outliers[['Country', 'Year', col]].head(5)\n",
    "            print(f\"  Sample outlier cases:\")\n",
    "            for _, row in outlier_countries.iterrows():\n",
    "                print(f\"    {row['Country']} ({row['Year']}): {row[col]:.3f}\")\n",
    "\n",
    "# Outlier handling strategy\n",
    "print(f\"\\nOUTLIER HANDLING STRATEGY:\")\n",
    "print(\"For this analysis, we'll keep outliers because:\")\n",
    "print(\"  1. Happiness data reflects real-world extremes\")\n",
    "print(\"  2. Some countries genuinely have extreme values\")\n",
    "print(\"  3. Outliers provide valuable insights\")\n",
    "print(\"  4. We have sufficient data points\")\n",
    "\n",
    "print(f\"\\nOutlier Summary:\")\n",
    "total_outliers = sum(outliers_found.values())\n",
    "print(f\"  Total outliers across all columns: {total_outliers}\")\n",
    "print(f\"  Percentage of dataset: {(total_outliers / (len(df) * len(outlier_cols))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d19823d-1d3d-4e27-b1da-8222f71d6b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA TYPE OPTIMIZATION\n",
      "Current memory usage:\n",
      "  Total memory: 0.29 MB\n",
      "\n",
      "Optimizing data types...\n",
      "Data type optimization complete\n",
      "\n",
      "After optimization:\n",
      "  Total memory: 0.09 MB\n",
      "  Memory reduction: 69.2%\n",
      "\n",
      "Optimized data types:\n",
      "  Ranking: int64\n",
      "  Country: category\n",
      "  Regional indicator: category\n",
      "  Happiness score: float32\n",
      "  GDP per capita: float32\n",
      "  Social support: float32\n",
      "  Healthy life expectancy: int64\n",
      "  Freedom to make life choices: float32\n",
      "  Generosity: float32\n",
      "  Perceptions of corruption: float32\n",
      "  Year: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: DATA TYPE OPTIMIZATION\n",
    "# Optimizing data types for better performance and memory usage\n",
    "\n",
    "print(\"\\nDATA TYPE OPTIMIZATION\")\n",
    "\n",
    "print(\"Current memory usage:\")\n",
    "memory_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"  Total memory: {memory_before:.2f} MB\")\n",
    "\n",
    "# Optimizing data types\n",
    "print(f\"\\nOptimizing data types...\")\n",
    "\n",
    "# Converting Year to integer if it's not already\n",
    "if df['Year'].dtype != 'int64':\n",
    "    df['Year'] = df['Year'].astype('int64')\n",
    "\n",
    "# Converting Ranking to integer if it's not already  \n",
    "if 'Ranking' in df.columns and df['Ranking'].dtype != 'int64':\n",
    "    df['Ranking'] = df['Ranking'].astype('int64')\n",
    "\n",
    "# Optimizing float columns (float64 to float32 if precision allows)\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    # Check if values fit in float32 range\n",
    "    if df[col].min() >= np.finfo(np.float32).min and df[col].max() <= np.finfo(np.float32).max:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "# Convert categorical columns to category type for memory efficiency\n",
    "categorical_cols = ['Country', 'Regional indicator']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "print(\"Data type optimization complete\")\n",
    "\n",
    "print(f\"\\nAfter optimization:\")\n",
    "memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"  Total memory: {memory_after:.2f} MB\")\n",
    "print(f\"  Memory reduction: {((memory_before - memory_after) / memory_before) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nOptimized data types:\")\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb86cc8-42fd-4826-871a-c7c8d1901ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING DERIVED FEATURES\n",
      "Creating new analytical features...\n",
      "Created 'Happiness Category' feature\n",
      "Created 'Decade' feature\n",
      "Created 'Period' feature\n",
      "Created 'GDP Category' feature\n",
      "Created 'Regional Happiness Rank' feature\n",
      "Created 'Happiness Change' feature\n",
      "\n",
      "New features created:\n",
      "Happiness Category\n",
      "Decade\n",
      "Period\n",
      "GDP Category\n",
      "Regional Happiness Rank\n",
      "Happiness Change\n",
      "\n",
      "Dataset now has 17 columns (was 11)\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: CREATING DERIVED FEATURES\n",
    "# Generating additional useful columns for analysis\n",
    "\n",
    "print(\"\\nCREATING DERIVED FEATURES\")\n",
    "\n",
    "print(\"Creating new analytical features...\")\n",
    "\n",
    "# 1. Happiness categories\n",
    "def categorize_happiness(score):\n",
    "    if score >= 7.0:\n",
    "        return 'Very Happy'\n",
    "    elif score >= 6.0:\n",
    "        return 'Happy'\n",
    "    elif score >= 5.0:\n",
    "        return 'Moderate'\n",
    "    elif score >= 4.0:\n",
    "        return 'Unhappy'\n",
    "    else:\n",
    "        return 'Very Unhappy'\n",
    "\n",
    "df['Happiness Category'] = df['Happiness score'].apply(categorize_happiness)\n",
    "print(\"Created 'Happiness Category' feature\")\n",
    "\n",
    "# 2. Decade indicator\n",
    "df['Decade'] = (df['Year'] // 10) * 10\n",
    "print(\"Created 'Decade' feature\")\n",
    "\n",
    "# 3. Year period (early, mid, late)\n",
    "def get_period(year):\n",
    "    if year <= 2017:\n",
    "        return 'Early (2015-2017)'\n",
    "    elif year <= 2020:\n",
    "        return 'Mid (2018-2020)'\n",
    "    else:\n",
    "        return 'Late (2021-2024)'\n",
    "\n",
    "df['Period'] = df['Year'].apply(get_period)\n",
    "print(\"Created 'Period' feature\")\n",
    "\n",
    "# 4. GDP category\n",
    "df['GDP Category'] = pd.cut(df['GDP per capita'], \n",
    "                           bins=4, \n",
    "                           labels=['Low GDP', 'Lower-Mid GDP', 'Upper-Mid GDP', 'High GDP'])\n",
    "print(\"Created 'GDP Category' feature\")\n",
    "\n",
    "# 5. Regional happiness rank\n",
    "df['Regional Happiness Rank'] = df.groupby(['Regional indicator', 'Year'])['Happiness score'].rank(ascending=False)\n",
    "print(\"Created 'Regional Happiness Rank' feature\")\n",
    "\n",
    "# 6. Year-over-year happiness change\n",
    "df = df.sort_values(['Country', 'Year'])\n",
    "df['Happiness Change'] = df.groupby('Country')['Happiness score'].diff()\n",
    "print(\"Created 'Happiness Change' feature\")\n",
    "\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = ['Happiness Category', 'Decade', 'Period', 'GDP Category', \n",
    "               'Regional Happiness Rank', 'Happiness Change']\n",
    "for feature in new_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"{feature}\")\n",
    "\n",
    "print(f\"\\nDataset now has {df.shape[1]} columns (was {df_raw.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b688ff6-7b71-4811-8287-eb4abffef810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA VALIDATION AFTER CLEANING\n",
      "Final data quality checks...\n",
      "Missing values: 181\n",
      "Duplicate country-year combinations: 0\n",
      "\n",
      "Value ranges after cleaning:\n",
      "Happiness score: 1.721 to 7.842\n",
      "Years: 2015 to 2024\n",
      "Countries: 175\n",
      "Regions: 10\n",
      "\n",
      "Data distribution:\n",
      "Total observations: 1,502\n",
      "Countries per year (avg): 150.2\n",
      "Years per country (avg): 8.6\n",
      "\n",
      "New feature validation:\n",
      "Happiness categories: {'Moderate': 466, 'Unhappy': 368, 'Happy': 363, 'Very Unhappy': 161, 'Very Happy': 144}\n",
      "Periods: {'Late (2021-2024)': 570, 'Early (2015-2017)': 470, 'Mid (2018-2020)': 462}\n",
      "Non-null happiness changes: 1327\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: DATA VALIDATION\n",
    "# Verifying data quality after cleaning\n",
    "\n",
    "print(\"\\nDATA VALIDATION AFTER CLEANING\")\n",
    "\n",
    "print(\"Final data quality checks...\")\n",
    "\n",
    "# 1. Checking for missing values\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f\"Missing values: {missing_after}\")\n",
    "\n",
    "# 2. Checking for duplicates\n",
    "duplicates_after = df.duplicated(['Country', 'Year']).sum()\n",
    "print(f\"Duplicate country-year combinations: {duplicates_after}\")\n",
    "\n",
    "# 3. Checking data ranges\n",
    "print(f\"\\nValue ranges after cleaning:\")\n",
    "print(f\"Happiness score: {df['Happiness score'].min():.3f} to {df['Happiness score'].max():.3f}\")\n",
    "print(f\"Years: {df['Year'].min()} to {df['Year'].max()}\")\n",
    "print(f\"Countries: {df['Country'].nunique()}\")\n",
    "print(f\"Regions: {df['Regional indicator'].nunique()}\")\n",
    "\n",
    "# 4. Checking data distribution\n",
    "print(f\"\\nData distribution:\")\n",
    "print(f\"Total observations: {len(df):,}\")\n",
    "print(f\"Countries per year (avg): {len(df) / df['Year'].nunique():.1f}\")\n",
    "print(f\"Years per country (avg): {len(df) / df['Country'].nunique():.1f}\")\n",
    "\n",
    "# 5. Validating new features\n",
    "print(f\"\\nNew feature validation:\")\n",
    "print(f\"Happiness categories: {df['Happiness Category'].value_counts().to_dict()}\")\n",
    "print(f\"Periods: {df['Period'].value_counts().to_dict()}\")\n",
    "print(f\"Non-null happiness changes: {df['Happiness Change'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e788a16c-2e0e-42d9-8296-773dfe27edae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVING CLEANED DATASET\n",
      "Cleaned dataset saved to: ../data/processed/happiness_cleaned.csv\n",
      "\n",
      "CLEANING SUMMARY:\n",
      "Original dataset: 1,502 rows × 11 columns\n",
      "Cleaned dataset: 1,502 rows × 17 columns\n",
      "Data retention: 100.0%\n",
      "Countries: 175\n",
      "Years: 10\n",
      "Missing values: 181\n",
      "Duplicates: 0\n",
      "Memory usage: 0.30 MB\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: SAVING CLEANED DATASET\n",
    "# Exporting the cleaned data for use in subsequent analysis\n",
    "\n",
    "print(\"\\nSAVING CLEANED DATASET\")\n",
    "\n",
    "# Saving cleaned dataset\n",
    "output_path = '../data/processed/happiness_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "\n",
    "# Saved data cleaning summary\n",
    "summary_stats = {\n",
    "    'original_rows': len(df_raw),\n",
    "    'cleaned_rows': len(df),\n",
    "    'original_columns': len(df_raw.columns),\n",
    "    'cleaned_columns': len(df.columns),\n",
    "    'countries': df['Country'].nunique(),\n",
    "    'years': df['Year'].nunique(),\n",
    "    'missing_values': df.isnull().sum().sum(),\n",
    "    'duplicates': df.duplicated(['Country', 'Year']).sum(),\n",
    "    'memory_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "}\n",
    "\n",
    "print(f\"\\nCLEANING SUMMARY:\")\n",
    "print(f\"Original dataset: {summary_stats['original_rows']:,} rows × {summary_stats['original_columns']} columns\")\n",
    "print(f\"Cleaned dataset: {summary_stats['cleaned_rows']:,} rows × {summary_stats['cleaned_columns']} columns\")\n",
    "print(f\"Data retention: {(summary_stats['cleaned_rows'] / summary_stats['original_rows']) * 100:.1f}%\")\n",
    "print(f\"Countries: {summary_stats['countries']}\")\n",
    "print(f\"Years: {summary_stats['years']}\")\n",
    "print(f\"Missing values: {summary_stats['missing_values']}\")\n",
    "print(f\"Duplicates: {summary_stats['duplicates']}\")\n",
    "print(f\"Memory usage: {summary_stats['memory_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4599827-7ab2-493c-8913-49bc152fba56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
